---
title: "Grupo Bimbo Inventory Demand"
subtitle: "Maximize sales and minimize returns of bakery goods"
author: "Gustavo Moreira"
date: "22/07/2019"
output: 
  html_document:
    toc: TRUE
    css: 'apresentacao.css'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Este trabalho foi sugerido pela [Data Science Academy](https://www.datascienceacademy.com.br/) durante o curso 'Formação Cientista de Dados'. Os dados utilizados estão disponíveis na competição [Grupo Bimbo Inventory Demand](https://www.kaggle.com/c/grupo-bimbo-inventory-demand/overview)

# Introdução

O Grupo Bimbo fornece produtos de panificação para diversos clientes em todo o México, atendendo aproximadamente 1 milhão de lojas ao longo de 45.000 rotas diferentes por este país. Atender a demanda adequada para cada loja é um grande desafio, visto que estes produtos possuem um prazo de validade de apenas uma semana, bem como os consumidores desejam comprar produtos frescos.

Há uma linha tênue entre fornecer produtos de menos, resultando em prateleiras vazias, ou fornecer produtos demais, resultando em produtos não vendidos e consequentemente fora do prazo de validade, causando prejuízo financeiro ao o lojista e à empresa. Atualmente os cálculos de inventário são realizados manualmente pelos funcionários responsáveis pelas entregas, que precisam prever a oferta e demanda de cada loja a partir de suas experiências pessoais.

O desafio é propor um modelo de Machine Learning capaz prever a demanda por novos produtos, de forma a maximizar as vendas e minimizar o retorno destes produtos.

# Definindo o problema de negócio

**Objetivo**: Prever a demanda adequada de produtos para cada requisição realizada por cada lojista.

**Observações importantes**:

* É possível que existam requisições para clientes e/ou produtos não documentados anteriormente. O modelo preditivo deve ser capaz de lidar com situações inéditas;

* Existem registros duplicados na tabela de clientes. Será necessário observar estes registros e determinar quais ações devem ser tomadas;

# Observação dos dados disponíveis

```{r libs, results='hide', include=FALSE, message=FALSE, warning=FALSE}
require(tidyverse)
require(data.table)
require(lubridate)
require(scales)
require(reshape2)
require(caret)
require(MLmetrics)
```

```{r files}
input_dir <- '../input'
files <- list.files('../input/', '.csv')

sapply(paste(input_dir, files, sep = '/'), file.info)
```

Foram disponibilizados os seguintes dados:

* cliente_tabla.csv: Tabela com ID e nome de clientes;

* producto_tabla.csv: Tabela com ID e nome de produtos;

* town_state.csv: Tabela com ID e nome de cidades e estados;

* sample_submission: Template de output para testar modelo preditivo no Kaggle;

* test.csv: Dados para teste;

* train.csv: Dados para treino.

```{r datasets_sample, results='hide', message=FALSE, warning=FALSE}
datasets = lapply(files, function(x){
  read_csv(paste(input_dir, x, sep = '/'), n_max = 100)
})
```

```{r datasets_sample_1}
head(datasets[[1]], 20)
```

Observando os primeiros registros, é possível detectar algumas inconsistências:

* O cliente 4 está duplicado;
 
* Os clientes 0 e 2 possuem o mesmo nome, sugerindo que não podemos agrupar clientes pelo nome dada a possibilidade de existir clientes homônimos.

Será necessário verificar se existe mais de um registro para o mesmo cliente, ou até mesmo um único ID para clientes diferentes.

```{r datasets_sample_2}
head(datasets[[2]], 20)
```

Os produtos estão corretamente cadastrados. O ID 0 aparenta ser um registro genérico para produtos não identificados previamente.

Cada produto possui informações extras em seu nome, como a quantidade de gramas (567g) e a quantidade de "peças" por pacote (4p). Podemos extrair estes dados para buscar mais características de cada produto.


```{r datasets_sample_5}
head(datasets[[5]], 20)
```

Existe um ID primário para cada distribuidora e um ID secundário para cada cidade. Ambos registros estão associados a um estado do México. 

```{r datasets_sample_3}
head(datasets[[3]], 20)
```

Template a ser submetido ao Kaggle.

```{r datasets_sample_4}
head(datasets[[4]], 20)
```

Descrição das variáveis disponíveis para teste:

* Semana: Número da semana no ano;

* Agencia_ID: ID da distribuidora, podendo ser associado ao dataset town_state.csv;

* Canal_ID: Meio de comunicação utilizado entre cliente e distribuidora;

* Ruta_SAK: ID do percurso realizado entre a distribuidora e o cliente;

* Cliente_ID: ID do cliente, podendo ser associado ao dataset cliente_tabla.csv;

* Producto_ID: ID do produto, podendo ser associado ao dataset producto_tabla.csv.


```{r datasets_sample_6}
head(datasets[[6]], 20)
```

Além dos dados de teste, temos disponível os valores necessários para calcular as features do modelo preditivo:

* Venta_uni_hoy: Número de vendas nesta semana;

* Venta_hoy: Valor financeiro obtido pelas vendas;

* Dev_uni_proxima: Número de devoluções na próxima semana;

* Dev_proxima: Valor financeiro das devoluções;

* Demanda_uni_equil: Variável a ser prevista, representando a demanda ajustada (Venta_uni_hoy - Dev_uni_proxima).

```{r cleanup, include=FALSE, echo=FALSE}
rm(datasets)
```

# Análise exploratória e engenharia de atributos

## Produtos

```{r analysis_products}
tbl_products <- read_csv(paste(input_dir, files[2], sep = '/'))
glimpse(tbl_products)
```

Cada produto possui algumas informações extras no nome:

* Peso do produto em gramas ou kilogramas e litros ou mililitros;

* Quantidade de peças por unidade.

Podemos extrair estes dados e observar se há correlação entre essas variáveis e a demanda por produtos:

```{r product_pipeline}
#As informações do peso e unidades de cada produto serão extraídas com Regex
extract_from_product_name <- function(tib){
  pattern_metric_weight <- '\\d+\\s*g|\\d+\\s*kg|\\d+\\s*ml|\\d+\\s*l'
  pattern_numeric_amount <- '\\d+p'
  pattern_product_first_name <- '^\\w+'
  return(tib %>%
           mutate(product_weight = tolower(str_extract(NombreProducto, regex(pattern_metric_weight, ignore_case = T))),
                  product_units = str_extract(NombreProducto, regex(pattern_numeric_amount, ignore_case = T)),
                  product_units = as.numeric(str_extract(product_units, '\\d*')),
                  product_first_name = str_extract(NombreProducto, pattern_product_first_name)))
}

#Função para padronizar o peso do produto em gramas e mililitros
standardize_weight = function(tib){
  metrics = data.frame(measure = c('g', 'kg', 'ml', 'l'),
                       value = c(1, 1000, 1, 1000))
  
  return(tib %>%
           mutate(measure = str_extract(product_weight, '[:alpha:]+'),
                  product_weight = as.numeric(str_extract(product_weight, '\\d*'))) %>%
           left_join(metrics, by = 'measure') %>%
           mutate(product_weight = product_weight * value) %>%
           select(-measure, -value))
}

#Valores NA na coluna 'product_units' serão substituídos por 1, indicando que o produto é unitário
impute_constant_unit = function(tib){
  return(tib %>% mutate(product_units = replace_na(product_units, 1)))
}

#Valores NA na coluna 'product_weight' serão substituídos pela mediana de seus grupos
impute_group_median_weight = function(tib){
  
  #Criamos um array com os produtos de valor missing
  na_weight_types <- tib %>%
    filter(is.na(product_weight)) %>%
    group_by(product_first_name) %>%
    summarize %>%
    flatten_chr
  
  #Para cada grupo, calculamos a mediana de peso de itens semelhantes
  df_median <- tib %>%
    filter(product_first_name %in% na_weight_types) %>%
    group_by(product_first_name) %>%
    summarize(median_weight = median(product_weight, na.rm = T)) %>%
    as.data.frame
  
  #E então, substituímos os valores NA a partir das medianas calculadas por grupo
  tib_list <- bind_rows(apply(df_median, 1, function(x){
    tib %>%
      filter(product_first_name == x[1]) %>%
      mutate(product_weight = replace_na(product_weight, as.numeric(x[2])))
  }))
  
  #Colocamos os valores imputados de volta no tibble inicial
  imputed_tbl <- left_join(tib, tib_list, by = 'Producto_ID') %>%
    mutate(product_weight.x = ifelse(is.na(product_weight.x), product_weight.y, product_weight.x)) %>%
    transmute(Producto_ID = Producto_ID,
              NombreProducto = NombreProducto.x,
              product_weight = product_weight.x,
              product_units = product_units.x)
  return(imputed_tbl)
}

#É provável que não seja possível imputar a mediana do peso do grupo em casos específicos, onde não há
#amostras suficientes para calcular a mediana. 
#Nessas situações, será imputado o peso médio
impute_mean_weight = function(tib){
  overall_mean_weight = round(mean(tib$product_weight, na.rm = T))
  return(tib %>% mutate(product_weight = replace_na(product_weight, overall_mean_weight)))
}

#Colocamos as etapas em um pipeline
product_pipeline = function(tib){
  steps = extract_from_product_name(tib) %>%
    standardize_weight %>%
    impute_constant_unit %>%
    impute_group_median_weight %>%
    impute_mean_weight
  return(steps)
}
```

```{r product_info, warning=FALSE}
tbl_product_info <- product_pipeline(tbl_products)
head(tbl_product_info, 20)
```

## Clientes

```{r analysis_clients}
tbl_clients <- read_csv(paste(input_dir, files[1], sep = '/'))
glimpse(tbl_clients)
```

Observando os primeiros registros de clientes, podemos perceber algumas inconsistências:

 * Existem registros duplicados, como o ID 4;
 
 * Existem clientes não identificados, cujos nomes foram preenchidos com um valor genérico 'SIN NOMBRE' (sem nome).

```{r duplicated_clients}
duplicated_clients <- tbl_clients %>%
  count(Cliente_ID) %>%
  filter(n > 1) %>%
  select(Cliente_ID) %>%
  flatten_dbl
length(duplicated_clients)
```

Dos 935.362 clientes, 4.862 aparecem duplicados. Precisamos verificar se estes registros referem-se ao mesmo cliente cadastrado mais de uma vez ou a clientes distintos compartilhando o mesmo ID:

```{r distinct_duplicated_clients}
tbl_duplicated_clients <- tbl_clients %>% filter(Cliente_ID %in% duplicated_clients)

#Utilizaremos regex para 'regularizar' espaços em branco e caracteres especiais, de forma a identificar clientes distintos com IDs iguais
pattern_trim_whitespace = '\\s+'
pattern_replace_punct = '[:punct:]+'
tbl_duplicated_clients <- tbl_duplicated_clients %>%
  mutate(eval_client_name = str_replace_all(NombreCliente, pattern_trim_whitespace, '_'),
         eval_client_name = str_replace_all(eval_client_name, pattern_replace_punct, '_')) %>%
  group_by(Cliente_ID, eval_client_name) %>% summarize %>% ungroup

#Buscando clientes distintos com o mesmo ID
tbl_duplicated_clients %>%
  count(Cliente_ID) %>%
  filter(n > 1) %>%
  select(Cliente_ID) %>%
  left_join(tbl_clients, by = 'Cliente_ID')
```

Após isolar espaços em branco e caracteres de pontuação do nome, agrupamos os registros por ID e conseguimos identificar quais clientes distintos estão compartilhando o mesmo ID.

Neste caso, foi identificado apenas o cliente 'LA CHIQUITA'. Como o nome 'SIN NOMBRE' está sendo utilizado de forma genérica para denominar clientes não identificados, podemos assumir que os registros duplicados neste dataset referem-se ao mesmo cliente. Portanto, nenhuma ação de regularização será tomada sobre estes dados.

## Cidades

```{r analysis_towns}
tbl_towns <- read_csv(paste(input_dir, files[5], sep = '/'))
glimpse(tbl_towns)
```

As informações de cidades e estados podem ser codificadas e representadas por IDs, a partir dos quais podemos conhecer a demanda média por cidade e estado:

```{r encode_towns}
encode_towns = function(tib){
  return(tib %>%
           mutate(Town_ID = as.numeric(factor(Town)),
                  State_ID = as.numeric(factor(State))) %>%
           select(Agencia_ID, Town_ID, State_ID))
}
head(encode_towns(tbl_towns),20)
```

## Histórico de transações

```{r train_data_size}
system('wc -l ../input/train.csv')
```

Existem cerca de 74 milhões de registros históricos. Serão extraídas aleatoriamente 500.000 amostras de cada semana observada:

```{r gather_sample}
tbl_train <- fread(paste(input_dir, files[6], sep = '/'))
week_range <- range(tbl_train$Semana)
week_sequence <- seq(week_range[1], week_range[2], 1)

set.seed(3)
train_sample <- bind_rows(lapply(week_sequence, function(x){
  tbl_train[Semana == x][sample(.N, 500000)]
})) %>% as_tibble
rm(tbl_train)

#Análise estatística
summary(train_sample)
```

As informações que temos disponíveis para extrair informações são IDs de diferentes aspectos de cada registro: distribuidora, canal de comunicação, produto solicitado, etc.

Além dos IDs, temos informações sobre a quantidade de vendas e devoluções realizadas. A variável que estamos tentando prever é a "Demanda_uni_equil", que é calculada da seguinte forma:

$$ D = Venta - Devol, D \geqslant 0 $$

* $Venta$ = Produtos vendidos na semana (Venta_uni_hoy);

* $Devol$ = Produtos a devolver na semana seguinte (Dev_uni_proxima)

É possível que existam registros em que devoluções superam vendas. Nestes casos, a demanda sempre será igual a zero.

## Visualizações

Para compreender melhor as informações disponíveis, faremos uma série de apresentações combinando a demanda com as demais variáveis.

### Semana

```{r plotdata_semana}
plotdata_semana <- train_sample %>%
  group_by(Semana) %>%
  summarize(sum_venta = sum(Venta_uni_hoy, na.rm = T),
            sum_devol = sum(Dev_uni_proxima, na.rm = T),
            sum_demanda = sum(Demanda_uni_equil, na.rm = T))
```


#### Vendas por semana

```{r plotdata_semana_venta}
ggplot(data = plotdata_semana) +
  geom_col(mapping = aes(x = as.factor(Semana),
                         y = sum_venta),
           fill = '#328cad', width = 0.65) +
  scale_y_continuous(labels = comma) +
  xlab('Semana') + ylab('Vendas') + theme_light() +
  ggtitle('Vendas por semana')
```


#### Devoluções por semana

```{r plotdata_semana_devol}
ggplot(data = plotdata_semana) +
  geom_col(mapping = aes(x = as.factor(Semana),
                         y = sum_devol),
           fill = '#ad3232', width = 0.65) +
  scale_y_continuous(labels = comma) +
  xlab('Semana') + ylab('Devoluções') + theme_light() +
  ggtitle('Devoluções por semana')
```

#### Demanda por semana

```{r plotdata_semana_demanda}
ggplot(data = plotdata_semana) +
  geom_col(mapping = aes(x = as.factor(Semana),
                         y = sum_demanda),
           fill = '#32ad32', width = 0.65) +
  scale_y_continuous(labels = comma) +
  xlab('Semana') + ylab('Demanda') + theme_light() +
  ggtitle('Demanda por semana')
```

* Nos dados observados, não há variação suficiente na demanda semanal para justificar o uso dessa variável.

* É possível observar um aumento no volume de devoluções a cada 4 semanas, sugerindo que exista um padrão comportamental periódico dos clientes. 

### Canal

```{r plotdata_channel}
plotdata_channel <- train_sample %>%
  count(Canal_ID, name = 'count_channel') %>%
  mutate(prop = (count_channel / sum(count_channel)) * 100)
```

#### Registros por canal

```{r plotdata_channel_usage}
ggplot(data = plotdata_channel,
       mapping = aes(x = reorder(as.factor(Canal_ID), count_channel),
                     y = count_channel,
                     fill = as.factor(Canal_ID))) +
  geom_col(color = '#777777', show.legend = F) +
  geom_text(mapping = aes(label = sprintf('%.2f%%', prop))) +
  scale_y_continuous(labels = comma) +
  coord_flip() + theme_light() +
  xlab('Canal') + ylab('Registros') +
  ggtitle('Registros por canal de comunicação')
```

90.9% das requisições são realizadas pelo canal 1, seguido pelo canal 4 com 5% das requisições. Os demais canais de comunicação concentram menos de 2% das requisições realizadas.

Após conhecer a proporção de uso destes canais, observamos a demanda média de cada registro.

#### Demanda por canal

```{r plotdata_channel_boxplot}
ggplot(data = train_sample %>% filter(Demanda_uni_equil > 0)) +
  geom_boxplot(mapping = aes(x = as.factor(Canal_ID), 
                             y = log(Demanda_uni_equil),
                             fill = as.factor(Canal_ID)),
               show.legend = F) +
  coord_flip() + theme_light() +
  xlab('Canal') + ylab('Demanda (log)') +
  ggtitle('Demandas por canal de comunicação')
```

* Este gráfico sugere que, em média, as demandas mais volumosas são atendidas pelos canais 5, 9 e 2. Contudo, no gráfico anterior, percebemos que estes canais são pouco utilizados;

* Mais importante, é possível detectar outliers de escala semelhante em todos canais de comunicação, sugerindo que não há uma distinção entre a demanda e o canal de comunicação.

### Distribuidora

```{r plotdata_depot}
plotdata_depot <- train_sample %>%
  group_by(Agencia_ID) %>%
  summarize(mean_depot_demanda = mean(Demanda_uni_equil),
            sum_depot_demanda = sum(Demanda_uni_equil)) %>%
  left_join(train_sample %>% count(Agencia_ID, name = 'count_agencia'), by = 'Agencia_ID') %>%
  left_join(encode_towns(tbl_towns), by = 'Agencia_ID')
```

#### Proporção de demanda por registro de distribuidora

```{r plotdata_depot_prop}
ggplot(data = plotdata_depot,
       mapping = aes(x = count_agencia,
                     y = sum_depot_demanda)) + geom_point() +
  xlab('Quantidade de registros') + ylab('Demanda total') + theme_light() +
  ggtitle('Proporção de demandas por registros')
```

Quando comparamos o total de demanda com a quantidade de registros, é esperada que exista uma correlação positiva (quanto mais registros atendidos, maior a demanda total). Contudo, este gráfico nos auxilia a encontrar alguns outliers do lado esquerdo: depósitos com baixo volume de registros e alto volume de demanda.

Podemos classificar estes registros como "distribuidoras de larga escala", sugerindo que registros efetuados para estas distribuidoras costumam resultar em alta demanda. Para isso, definimos um limiar de demanda média e quantidade média de registros atendidos:

```{r plotdata_depot_highlight}
large_scale_depot_mean_threshold = quantile(plotdata_depot$mean_depot_demanda, 0.75)
large_scale_depot_count_threshold = quantile(plotdata_depot$count_agencia, 0.5)

plotdata_depot <- plotdata_depot %>% 
  mutate(large_scale_depot = as.numeric(mean_depot_demanda >= large_scale_depot_mean_threshold &
                                          count_agencia >= large_scale_depot_count_threshold))

ggplot(data = plotdata_depot,
       mapping = aes(x = count_agencia,
                     y = sum_depot_demanda)) + 
  geom_point(mapping = aes(color = as.factor(large_scale_depot)),
             show.legend = F) +
  scale_color_manual(values = c('#c5dbc5', '#32ad32')) + 
  xlab('Quantidade de registros') + ylab('Total de demanda') + theme_light() +
  ggtitle('Proporção de demandas por registros')
```

```{r plotdata_depot_show}
plotdata_depot %>%
  filter(large_scale_depot == 1) %>%
  arrange(-mean_depot_demanda)
```

Em média, é esperado que as solicitações realizadas para estas distribuidoras sejam maiores que o usual.

### Cidade

```{r plotdata_town}
plotdata_town <- plotdata_depot %>%
  select(sum_depot_demanda, count_agencia, Town_ID) %>%
  group_by(Town_ID) %>%
  summarize(sum_town_demanda = sum(sum_depot_demanda),
            count_town_demanda = sum(count_agencia),
            mean_town_demanda = sum_town_demanda/count_town_demanda) %>% ungroup
```

#### Demanda total por cidade

```{r plotdata_town_top25, fig.height=6, fig.width=12}
ggplot(data = plotdata_town %>% top_n(25, sum_town_demanda),
       mapping = aes(x = reorder(as.factor(Town_ID), -sum_town_demanda),
                     y = sum_town_demanda)) +
  geom_col(color = '#777777', fill = '#32ad32') +
  scale_y_continuous(labels = comma) + theme_light() +
  xlab('Cidade ID') + ylab('Demanda total') +
  ggtitle('Demanda total por cidade - Top 25')
```

#### Demanda total e quantidade de distribuidoras por cidade

```{r plotdata_town_depot_top25, fig.height=6, fig.width=12}
depots_by_town <- plotdata_depot %>%
  count(Town_ID, name = 'num_depots')

ggplot(data = plotdata_town %>% 
         top_n(25, sum_town_demanda) %>%
         left_join(depots_by_town, by = 'Town_ID') %>%
         select(Town_ID, sum_town_demanda, num_depots) %>% melt(id = 'Town_ID'),
       mapping = aes(x = reorder(as.factor(Town_ID), -value),
                     y = value)) +
  geom_col(color = '#777777', fill = '#32ad32') +
  scale_y_continuous(labels = comma) + theme_light() +
  facet_wrap(facets = ~ variable,
             scales = 'free_y',
             nrow = 2, ncol = 1) +
  xlab('ID Cidade') + ylab('Demanda total') +
  ggtitle('Demanda total e quantidade de distribuidoras por cidade - Top 25')
```


```{r plotdata_town_depot_bottom25, fig.height=6, fig.width=12}
ggplot(data = plotdata_town %>% 
         top_n(-25, sum_town_demanda) %>%
         left_join(depots_by_town, by = 'Town_ID') %>%
         select(Town_ID, sum_town_demanda, num_depots) %>% melt(id = 'Town_ID'),
       mapping = aes(x = reorder(as.factor(Town_ID), -value),
                     y = value)) +
  geom_col(color = '#777777', fill = '#32ad32') +
  scale_y_continuous(labels = comma) + theme_light() +
  facet_wrap(facets = ~ variable,
             scales = 'free_y',
             nrow = 2, ncol = 1) +
  xlab('ID Cidade') + ylab('Demanda total') +
  ggtitle('Demanda total e quantidade de distribuidoras por cidade - Bottom 25')
```

Existe uma correlação entre a demanda por cidade e a quantidade de distribuidoras disponíveis. Cidades com maior demanda possuem mais distribuidoras, enquanto cidades com menor demanda possuem menos distribuidoras.

### Cliente

```{r plotdata_client}
plotdata_client <- train_sample %>%
  group_by(Cliente_ID) %>%
  summarize(mean_demanda_cliente = mean(Demanda_uni_equil),
            sum_demanda_cliente = sum(Demanda_uni_equil)) %>%
  left_join(tbl_clients, by = 'Cliente_ID')
```

#### Demanda média e demanda total por cliente

```{r plotdata_client_sum, fig.height=6, fig.width=15}
ggplot(data = plotdata_client %>%
         top_n(20, sum_demanda_cliente) %>%
         select(-Cliente_ID) %>% melt(id = 'NombreCliente'),
       mapping = aes(x = reorder(as.factor(NombreCliente), value),
                     y = value)) +
  geom_col(color = '#e7e7e7', fill = '#dff0df') + 
  geom_text(mapping = aes(label = sprintf('%.0f', value))) + 
  coord_flip() + theme_light() +
  facet_wrap(facets = ~ variable,
             scales = 'free_x',
             nrow = 1, ncol = 2) +
  xlab('Clientes') + ylab('Demanda média / Demanda total') +
  ggtitle('Demanda média e demanda total por cliente - Top 20')
```

Puebla Remision é um dos principais clientes do Grupo Bimbo, com aproximadamente 860.000 produtos vendidos neste conjunto de dados, valor muito superior aos demais clientes. 

Mesmo com essa presença excessiva, a demanda média deste cliente está bem próxima aos demais registros, ao passo que outros clientes possuem uma demanda média muito elevada, mas pouca demanda total registrada.

Uma possível justificativa para essa discrepância é a variedade de produtos vendidos. É possível que clientes com baixa variedade de produtos necessitam de um volume maior destes itens, enquanto clientes com uma variedade maior demandam uma quantidade menor de cada item.

### Produto

```{r plotdata_product, warning=FALSE}
plotdata_product <- train_sample %>%
  group_by(Producto_ID) %>%
  summarize(mean_producto_demanda = mean(Demanda_uni_equil),
            sum_producto_demanda = sum(Demanda_uni_equil)) %>%
  left_join(product_pipeline(tbl_products), by = 'Producto_ID')
```

#### Demanda média por peso do produto

```{r plotdata_product_mean_weight}
ggplot(data = plotdata_product,
       mapping = aes(x = product_weight,
                     y = mean_producto_demanda)) +
  geom_point() + theme_light() +
  xlab('Peso do produto') + ylab('Demanda média') +
  ggtitle('Demanda média por peso do produto')
```

Na maioria das ocorrências, não há uma correlação entre a demanda e o peso do produto. Contudo, é possível perceber que produtos mais pesados possuem uma demanda menor, enquanto produtos mais leves possuem uma demanda maior.

#### Demanda média por variedade de produtos por cliente

```{r plotdata_product_by_client}
product_by_client <- train_sample %>%
  group_by(Cliente_ID) %>%
  summarize(unique_product_count = n_distinct(Producto_ID))

ggplot(data = product_by_client %>% right_join(plotdata_client, by = 'Cliente_ID'),
       mapping = aes(x = unique_product_count,
                     y = mean_demanda_cliente)) +
  geom_point() + theme_light() + 
  xlab('Variedade de produtos') + ylab('Demanda média') +
  ggtitle('Demanda média por variedade de produtos por cliente')
```

Semelhante ao observado em relação ao peso do produto, não há uma correlação direta entre essas duas variáveis, porém é esperado uma demanda média menor à medida que produtos diferentes são solicitados pelo cliente.

```{r cleanup_2, include=FALSE, echo=FALSE}
rm(list=ls())
```

# Concepção do modelo preditivo

A partir das observações realizadas durante análise, decidi selecionar as seguintes features para compor o modelo preditivo:

* Demanda média por cliente;

* Demanda média por produto;

* Demanda média por cidade;

* Demanda média por distribuidora.

As médias serão calculadas em escala logarítmica. Além destas, também serão adicionadas features binárias:

* Distribuidora de larga escala;

* Produto é pesado ($p \geqslant 5000g$);

* Produto possui múltiplas unidades ($u \geqslant 6$).

```{r toolkit}
#Extrair métricas a partir do nome do produto
extract_from_product_name <- function(tib){
  pattern_metric_weight <- '\\d+\\s*g|\\d+\\s*kg|\\d+\\s*ml|\\d+\\s*l'
  pattern_numeric_amount <- '\\d+p'
  pattern_product_first_name <- '^\\w+'
  return(tib %>%
           mutate(product_weight = tolower(str_extract(NombreProducto, regex(pattern_metric_weight, ignore_case = T))),
                  product_units = str_extract(NombreProducto, regex(pattern_numeric_amount, ignore_case = T)),
                  product_units = as.numeric(str_extract(product_units, '\\d*')),
                  product_first_name = str_extract(NombreProducto, pattern_product_first_name)))
}

#Padronização de peso por produto
standardize_weight = function(tib){
  metrics = data.frame(measure = c('g', 'kg', 'ml', 'l'),
                       value = c(1, 1000, 1, 1000))
  
  return(tib %>%
           mutate(measure = str_extract(product_weight, '[:alpha:]+'),
                  product_weight = as.numeric(str_extract(product_weight, '\\d*'))) %>%
           left_join(metrics, by = 'measure') %>%
           mutate(product_weight = product_weight * value) %>%
           select(-measure, -value))
}

#Imputar valor constante na quantidade unitária de produtos
impute_constant_unit = function(tib){
  return(tib %>% mutate(product_units = replace_na(product_units, 1)))
}

#Imputar mediana no peso por grupo de produto
impute_group_median_weight = function(tib){

  #Criamos um array com os produtos de valor missing
  na_weight_types <- tib %>%
    filter(is.na(product_weight)) %>%
    group_by(product_first_name) %>%
    summarize %>%
    flatten_chr

  #Para cada grupo, calculamos a mediana de peso de itens semelhantes
  df_median <- tib %>%
    filter(product_first_name %in% na_weight_types) %>%
    group_by(product_first_name) %>%
    summarize(median_weight = median(product_weight, na.rm = T)) %>%
    as.data.frame

  #E então, substituímos os valores NA a partir das medianas calculadas por grupo
  tib_list <- bind_rows(apply(df_median, 1, function(x){
    tib %>%
      filter(product_first_name == x[1]) %>%
      mutate(product_weight = replace_na(product_weight, as.numeric(x[2])))
  }))

  #Colocamos os valores imputados de volta no tibble inicial
  imputed_tbl <- left_join(tib, tib_list, by = 'Producto_ID') %>%
    mutate(product_weight.x = ifelse(is.na(product_weight.x), product_weight.y, product_weight.x)) %>%
    transmute(Producto_ID = Producto_ID,
              NombreProducto = NombreProducto.x,
              product_weight = product_weight.x,
              product_units = product_units.x)
  return(imputed_tbl)
}

#Imputar média no peso do produto
impute_mean_weight = function(tib){
  overall_mean_weight = round(mean(tib$product_weight, na.rm = T))
  return(tib %>% mutate(product_weight = replace_na(product_weight, overall_mean_weight)))
}

#Calcular as features de produtos
calc_product_features = function(tib){
  
  #Produtos acima de 5 litros ou 5 kilogramas serão classificados como "produtos pesados"
  weight_threshold = 5000
  
  #Produtos com 6 ou mais unidades serão classificados como "produtos empacotados"
  bundle_threshold = 6
  
  return(tib %>%
           mutate(heavy_product = as.numeric(product_weight >= weight_threshold),
                  bundled_product = as.numeric(product_units >= 6)))
}

#Pipeline de produtos
product_pipeline = function(tib){
  steps = extract_from_product_name(tib) %>%
    standardize_weight %>%
    impute_constant_unit %>%
    impute_group_median_weight %>%
    impute_mean_weight %>%
    calc_product_features
  return(steps)
}

#### Cidades ####
#Aplicar label encoding nas cidades e estados
encode_towns = function(tib){
  return(tib %>%
           mutate(Town_ID = as.numeric(factor(Town)),
                  State_ID = as.numeric(factor(State))) %>%
           select(Agencia_ID, Town_ID, State_ID))
}

#### Feature Engineering ####
calc_train_features = function(train_data, product_data, town_data,
                               quantile_depot_mean_threshold = 0.75, 
                               quantile_depot_count_threshold = 0.5){
  
  #Produtos
  product_info = product_pipeline(product_data) %>%
    select(-NombreProducto, -product_weight, -product_units)
  
  #Cidades
  town_info = encode_towns(town_data)
  
  #Demanda por canal
  channels = train_data %>%
    group_by(Canal_ID) %>%
    summarize(mean_channel_demanda = mean(Demanda_uni_equil, na.rm = T)) %>% ungroup
  
  #Demanda por distribuidoras
  depots = train_data %>%
    group_by(Agencia_ID) %>%
    summarize(mean_depot_demanda = mean(Demanda_uni_equil)) %>%
    left_join(train_data %>% count(Agencia_ID, name = 'count_agencia'), by = 'Agencia_ID') %>%
    left_join(town_info, by = 'Agencia_ID')
  
  large_scale_depot_mean_threshold = quantile(depots$mean_depot_demanda, quantile_depot_mean_threshold)
  large_scale_depot_count_threshold = quantile(depots$count_agencia, quantile_depot_count_threshold)
  
  depots <- depots %>%
    mutate(large_scale_depot = as.numeric(mean_depot_demanda >= large_scale_depot_mean_threshold &
                                            count_agencia >= large_scale_depot_count_threshold)) %>%
    select(-count_agencia)
  
  #Demanda por cidade
  town_depots = train_data %>%
    left_join(town_info, by = 'Agencia_ID') %>%
    group_by(Town_ID) %>%
    summarize(mean_town_demanda = mean(Demanda_uni_equil))
  
  #Demanda por cliente
  clients = train_data %>%
    group_by(Cliente_ID) %>%
    summarize(mean_demanda_cliente = mean(Demanda_uni_equil)) %>%
    distinct
  
  #Demanda por produto
  products = train_data %>%
    group_by(Producto_ID) %>%
    summarize(mean_producto_demanda = mean(Demanda_uni_equil)) %>%
    left_join(product_info, by = 'Producto_ID')
  
  return(list(channels, depots, town_depots, clients, products))
}
```

## Carregando os dados de treino

```{r load_train, warning=FALSE, message=FALSE}
input_dir = '../input'

#Selecionando arquivos
files <- list.files(input_dir, '.csv')

#Produtos
tbl_products <- read_csv(paste(input_dir, files[2], sep = '/'))

#Cidades
tbl_towns <- read_csv(paste(input_dir, files[5], sep = '/'))

#Dados de treino
tbl_train <- fread(paste(input_dir, files[6], sep = '/'), select = c('Semana', 'Agencia_ID', 'Canal_ID', 'Cliente_ID', 'Producto_ID', 'Demanda_uni_equil'))

#Colocando a demanda em escala logarítmica
tbl_train[, Demanda_uni_equil := log1p(Demanda_uni_equil)]

#Serão selecionados 500.000 registros das semanas 5, 6, 7 e 8
set.seed(11)
train_sample <- bind_rows(lapply(5:8, function(x){
  tbl_train[Semana == x][sample(.N, 500000)]
})) %>% as_tibble

#Removemos o dataset completo para reduzir o espaço utilizado em memória
rm(tbl_train)

#Calculamos as features
features = calc_train_features(train_sample, tbl_products, tbl_towns)

#Agrupando features
for(f in features){
  train_sample <- train_sample %>% left_join(f)
}

#Retirando variáveis não utilizadas
train_sample <- train_sample %>%
  select(-Semana, -Agencia_ID, -Canal_ID, -Producto_ID, -Cliente_ID, -Town_ID, -State_ID)

#Será utilizado um modelo de regressão linear múltipla. O modelo será treinado com dados
#entre 1.5 desvios padrões da média
mean_threshold <- mean(train_sample$Demanda_uni_equil)
sd_threshold <- sd(train_sample$Demanda_uni_equil)

train_sample <- train_sample %>%
  filter(Demanda_uni_equil >= mean_threshold - (1.5 * sd_threshold) & 
         Demanda_uni_equil <= mean_threshold + (1.5 * sd_threshold))
```

## Criação do modelo

Serão utilizados dois modelos preditivos, regressão linear múltipla (lm) e gradiente estocástico descendente (gbm):

```{r model_lm}
set.seed(17)
t_index = createDataPartition(train_sample$Demanda_uni_equil, times = 1, p = 0.7, list = F)
train = train_sample[t_index,]
test = train_sample[-t_index,]

model_lm <- lm(Demanda_uni_equil ~ ., data = train)
summary(model_lm)
```

```{r model_gbm}
gbm_tuneGrid = expand.grid(n.trees = c(10, 20, 30),
                           interaction.depth = 1,
                           shrinkage = c(0.1, 1),
                           n.minobsinnode = 100)
gbm_trControl = trainControl(number = 1, search = 'grid', verboseIter = F)
model_gbm = train(Demanda_uni_equil ~ .,
                  data = train,
                  method = 'gbm',
                  tuneGrid = gbm_tuneGrid,
                  trControl = gbm_trControl)
summary(model_gbm)
```

Este modelo sofreu uma influência muito significativa da média por cliente e a média por produto, sugerindo que o uso dessas duas variáveis resultará em um modelo tendencioso e com baixa capacidade de generalização, especialmente se utilizado para realizar previsões sobre um cliente ou produto deconhecidos.

## Teste do modelo

Com os modelos criados, realizamos previsões nos dados de teste e avaliamos a performance utilizando a métrica RMSLE:

```{r predict_model_lm}
#Como a demanda sempre será um valor maior ou igual a zero, valores negativos serão substituídos por zero
preds_lm <- predict.lm(model_lm, test)
preds_lm <- replace(preds_lm, preds_lm < 0, 0)
RMSLE(preds_lm, test$Demanda_uni_equil)
```

```{r predict_model_gbm}
preds_gbm <- predict.train(model_gbm, test)
preds_gbm <- replace(preds_gbm, preds_gbm < 0, 0)
RMSLE(preds_gbm, test$Demanda_uni_equil)
```

Os dois modelos apresentaram performances semelhantes neste conjunto de dados. Faremos uma avaliação final utilizando os dados de teste da competição.

## Submetendo resultados ao Kaggle

```{r kaggle_submission, eval=FALSE}
kaggle_test_data = fread(paste(input_dir, files[4], sep = '/'))
kaggle_test_data <- merge(kaggle_test_data, features[[1]], all.x = T, by = 'Canal_ID')
kaggle_test_data <- merge(kaggle_test_data, features[[2]], all.x = T, by = 'Agencia_ID')
kaggle_test_data <- merge(kaggle_test_data, features[[3]], all.x = T, by = 'Town_ID')
kaggle_test_data <- merge(kaggle_test_data, features[[4]], all.x = T, by = 'Cliente_ID')
kaggle_test_data <- merge(kaggle_test_data, features[[5]], all.x = T, by = 'Producto_ID')

#Verificando valores NA
sapply(kaggle_test_data, function(x) sum(is.na(x)))

#Nas features relacionadas a médias já calculadas, será imputada a mediana.
#Nas demais features binárias, será imputado um valor constante zero
kaggle_test_data[is.na(mean_depot_demanda)]$mean_depot_demanda <- median(kaggle_test_data$mean_depot_demanda, na.rm = T)
kaggle_test_data[is.na(mean_town_demanda)]$mean_town_demanda <- median(kaggle_test_data$mean_town_demanda, na.rm = T)
kaggle_test_data[is.na(mean_demanda_cliente)]$mean_demanda_cliente <- median(kaggle_test_data$mean_demanda_cliente, na.rm = T)
kaggle_test_data[is.na(mean_producto_demanda)]$mean_producto_demanda <- median(kaggle_test_data$mean_producto_demanda, na.rm = T)
kaggle_test_data[is.na(large_scale_depot)]$large_scale_depot <- 0
kaggle_test_data[is.na(heavy_product)]$heavy_product <- 0
kaggle_test_data[is.na(bundled_product)]$bundled_product <- 0

submission_preds_lm <- predict.lm(model_lm, kaggle_test_data)
submission_preds_lm <- replace(submission_preds_lm, submission_preds_lm < 0, 0)

submission_preds_gbm <- predict.train(model_gbm, kaggle_test_data)
submission_preds_gbm <- replace(submission_preds_gbm, submission_preds_gbm < 0, 0)

#Colocando as previsões na escala original
submission_preds_lm <- expm1(submission_preds_lm)
submission_preds_gbm <- expm1(submission_preds_gbm)

#Carregando sample submission e submetendo resultados
submission = fread(paste(input_dir, files[3], sep = '/'))

submission[, Demanda_uni_equil := submission_preds_lm]
fwrite(submission, '../output/sample_submission_lm.csv')

submission[, Demanda_uni_equil := submission_preds_gbm]
fwrite(submission, '../output/sample_submission_gbm.csv')
```

![Resultados dos modelos: 'gbm' acima e 'lm' abaixo](../output/sample_submission.png)

Os dois modelos foram capazes de efetuar previsões sobre os dados, porém com uma taxa de erros muito maior que o observado durante a fase de teste.

Ambos modelos apresentaram uma performance pior no score privado, sugerindo que este modelo não obteve capacidade de generalização quando confrontado com dados inéditos.

Este trabalho ainda pode ser aperfeiçoado das seguintes formas:

* Reiniciar a análise exploratória em busca de novas features;

* Selecionar um volume maior de dados, possivelmente abrangendo todos períodos observados;

* Testar outros modelos preditivos.