---
title: "TalkingData"
subtitle: "AdTracking Fraud Detection Challenge"
author: "Gustavo Moreira"
date: "13/07/2019"
output: 
  html_document:
    toc: TRUE
    css: 'apresentacao.css'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Este trabalho foi sugerido pela [Data Science Academy](https://www.datascienceacademy.com.br/) durante o curso 'Formação Cientista de Dados'. Os dados utilizados estão disponíveis na competição [TalkingData AdTracking Fraud Detection Challenge](https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/overview)

# Introdução

TalkingData é a plataforma de serviços de Big Data mais abrangente da China, cobrindo cerca de 70% dos dispositivos móveis desta nação. Eles recebem dados de aproximadamente 3 bilhões de cliques diários em anúncios de aplicativos(apps). Deste volume, estima-se que 90% dos cliques são potencialmente fraudulentos.

Por fraude, entende-se a realização de cliques em anúncios de apps com a finalidade de elevar o volume de acessos de um canal de comunicação. Dessa forma, o custo financeiro destes canais aumenta, sem representar necessariamente cliques efetuados por usuários reais.

A atual implementação desta empresa para coibir cliques fraudulentos é medir, para cada usuário, a navegação realizada através dos produtos de seu portfólio. A partir dessa métrica, são identificados os IPs que efetuam uma grande quantidade de cliques mas nunca realizaram o download do app anunciado. Registros classificados como fraudulentos são adicionados a uma lista negra de IPs e dispositivos.

A detecção de fraude apresentou ótimos resultados e agora a TalkingData está buscando aprimorar essa implementação: deseja-se prever se um clique realizado resultará no download do app anunciado.

# Definindo o problema de negócio

**Objetivo:** Prever se um clique resultará no download de um app. Cada registro será **classificado** como 0 (não realizou download) e 1 (realizou download)

**Premissas:** Para desenvolver essa solução, será verdadeira a premissa de que os dados disponíveis representam cliques de usuários reais, portanto, não classificados como fraudulentos.

# Observação dos dados disponíveis

```{r libs, results='hide', include=FALSE, message=FALSE, warning=FALSE}
require(tidyverse)
require(lubridate)
require(corrplot)
require(caret)
```

```{r files}
input_dir = '../input'

files = list.files(input_dir, '.csv')
sapply(paste(input_dir, files, sep = '/'), file.info)
```

Foram disponibilizados os seguintes arquivos:

* train.csv: Dados para treino. Devido ao tamanho deste arquivo, será difícil carregá-lo completamente em memória;

* train_sample.csv: 100.000 amostras extraídas aleatoriamente de train.csv;

* test.csv, test_supplement.csv: Dados para teste;

* sample_submission: Template para avaliar resultado do modelo preditivo.

Serão utilizados os dados de train_sample para análise:

```{r train_sample, message=FALSE}
tbl_train_sample = read_csv(paste(input_dir, files[4], sep = '/'))
glimpse(tbl_train_sample)
```

* As variáveis ip, app, device, os e channel representam IDs, não devendo ser tratadas como variáveis contínuas;

* A variável is_attributed é o valor a ser previsto, sendo equivalente a 0 ou 1;

* click_time representa a data e hora do clique, enquanto attributed_time representa a data e hora do download.

Antes de prosseguir a análise, vamos verificar quais variáveis estão disponíveis para teste:

```{r test_sample, message=FALSE}
tbl_test_sample = read_csv(paste(input_dir, files[3], sep = '/'), n_max = 100)
glimpse(tbl_test_sample)
```

Todos IDs estão disponíveis para realizar as previsões. Click_ID será associado ao template do arquivo 'sample_submission' para avaliar a performance do modelo no Kaggle.

```{r cleanup, include=FALSE, echo=FALSE}
rm('tbl_test_sample')
```

# Análise exploratória

```{r summary}
summary(tbl_train_sample)
table(tbl_train_sample$is_attributed)
```

Os registros dessa análise foram coletados entre os dias 06/11/2017 e 09/11/2017. Dos 100.000 cliques observados, apenas 227 downloads foram efetuados. Isso sugere que a grande maioria dos cliques costumam não resultar em um download.

Para detectar as diferenças entre registros que resultam em um download, será necessário balancear a quantidade de registros de cada classe, de forma a obter uma distribuição equilibrada:

```{r undersample}
set.seed(1)
tbl_undersample <- bind_rows(tbl_train_sample %>% filter(is_attributed == 1),
                             tbl_train_sample %>% filter(is_attributed == 0) %>% sample_n(227))
glimpse(tbl_undersample)
```

## Feature Engineering

Apenas as variáveis disponíveis neste dataset não são suficientes para esclarecer se um clique resultará em um download. Com os dados disponíveis, pensei em calcular as seguintes métricas:

* Média de downloads por app, device, os e channel;

* Média de cliques por app, device, os e channel;

* Extrair componentes da data e hora do clique: dia da semana, dia, hora, minuto, segundo.

```{r util_functions}
## Granularizar o campo click_date
strip_date = function(tib){
  tbl_date <- tib %>% 
    transmute(click_day = day(click_time),
              click_dayofweek = as.POSIXlt(click_time)$wday,
              click_hour = hour(click_time),
              click_minute = minute(click_time),
              click_second = second(click_time))
  return(bind_cols(tib, tbl_date) %>% select(-click_time))
}

#Média de downloads por app
mean_app_downloads = function(tbl){
  return(tbl %>% 
           group_by(app) %>%
           summarize(mean_app_downloads = mean(is_attributed)) %>%
           ungroup)
}

#Média de downloads por dispositivo
mean_device_downloads = function(tbl){
  return(tbl %>% 
           group_by(device) %>%
           summarize(mean_device_downloads = mean(is_attributed)) %>%
           ungroup)
}

#Média de downloads por sistema operacional
mean_os_downloads = function(tbl){
  return(tbl %>% 
           group_by(os) %>%
           summarize(mean_os_downloads = mean(is_attributed)) %>%
           ungroup)
}

#Média de downloads por canal
mean_channel_downloads = function(tbl){
  return(tbl %>% 
           group_by(channel) %>%
           summarize(mean_channel_downloads = mean(is_attributed)) %>%
           ungroup)
}

#Média de cliques por aplicativo
mean_app_clicks = function(tbl){
  total_clicks = nrow(tbl)
  tbl_clicks = tbl %>% count(app)
  return(tbl_clicks %>%
           mutate(n = n / total_clicks) %>%
           rename(mean_app_clicks = n))
}

#Média de cliques por dispositivo
mean_device_clicks = function(tbl){
  total_clicks = nrow(tbl)
  tbl_clicks = tbl %>% count(device)
  return(tbl_clicks %>%
           mutate(n = n / total_clicks) %>%
           rename(mean_device_clicks = n))
}

#Média de cliques por sistema operacional
mean_os_clicks = function(tbl){
  total_clicks = nrow(tbl)
  tbl_clicks = tbl %>% count(os)
  return(tbl_clicks %>%
           mutate(n = n / total_clicks) %>%
           rename(mean_os_clicks = n))
}

#Média de cliques por canal
mean_channel_clicks = function(tbl){
  total_clicks = nrow(tbl)
  tbl_clicks = tbl %>% count(channel)
  return(tbl_clicks %>%
           mutate(n = n / total_clicks) %>%
           rename(mean_channel_clicks = n))
}
```

```{r means}
#Elaborando features com base em downloads
tbl_dl_app <- mean_app_downloads(tbl_undersample)
tbl_dl_device <- mean_device_downloads(tbl_undersample)
tbl_dl_os <- mean_os_downloads(tbl_undersample)
tbl_dl_channel <- mean_channel_downloads(tbl_undersample)

#Elaborando features com base em cliques
tbl_click_app <- mean_app_clicks(tbl_undersample)
tbl_click_device <- mean_device_clicks(tbl_undersample)
tbl_click_os <- mean_os_clicks(tbl_undersample)
tbl_click_channel <- mean_channel_clicks(tbl_undersample)
```

```{r merge}
tbl_undersample <- tbl_undersample %>%
  strip_date() %>%
  left_join(tbl_dl_app, by = 'app') %>%
  left_join(tbl_dl_device, by = 'device') %>%
  left_join(tbl_dl_os, by = 'os') %>%
  left_join(tbl_dl_channel, by = 'channel') %>%
  left_join(tbl_click_app, by = 'app') %>%
  left_join(tbl_click_device, by = 'device') %>%
  left_join(tbl_click_os, by = 'os') %>%
  left_join(tbl_click_channel, by = 'channel')
```

## Correlação entre features

Após calcular e associar cada média a seu ID e extrair os componentes de data e hora, verificamos a correlação entre as variáveis elaboradas:

```{r corr, fig.height=9, fig.width=9}
tbl_undersample_corr <- tbl_undersample %>%
  select(-ip, -app, -device, -os, -channel) %>%
  select_if(is.numeric) %>%
  cor()
corrplot(tbl_undersample_corr, method = ('square'))
```

Dessa matriz de correlação, podemos extrair as seguintes informações:

* O momento do clique possui pouca influência sobre a decisão do usuário realizar o download do app;

* As variáveis 'app' e 'channel' exercem uma influência maior na decisão do usuário realizar o download do app, ao passo que as variáveis 'device' e 'os' exercem uma influência menor;

* Há uma forte correlação negativa da variável device com as demais.

## Visualizações

Para buscar compreender melhor cada variável, será necessário elaborar algumas visualizações mais específicas.

### Downloads por app

```{r plot_app_downloads, fig.width=16, fig.height=5}
ggplot(data = tbl_undersample) +
  geom_histogram(mapping = aes(x = app),
                 bins = 1,
                 fill = '#004d05') +
  scale_x_continuous(position = 'top') +
  theme(panel.grid = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) +
  xlab('Apps') + ylab('Downloads') +
  ggtitle('Downloads por app') + 
  facet_grid(is_attributed ~ app)
```

### Downloads por device

```{r plot_device_downloads, fig.width=16, fig.height=5}
ggplot(data = tbl_undersample) +
  geom_histogram(mapping = aes(x = device),
                 bins = 1,
                 fill = '#004d05') +
  scale_x_continuous(position = 'top') +
  theme(panel.grid = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) +
  xlab('Device') + ylab('Downloads') +
  ggtitle('Downloads por device') + 
  facet_grid(is_attributed ~ device)
```

### Downloads por os

```{r plot_os_downloads, fig.width=16, fig.height=5}
ggplot(data = tbl_undersample) +
  geom_histogram(mapping = aes(x = os),
                 bins = 1,
                 fill = '#004d05') +
  scale_x_continuous(position = 'top') +
  theme(panel.grid = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) +
  xlab('Os') + ylab('Downloads') +
  ggtitle('Downloads por os') + 
  facet_grid(is_attributed ~ os)
```

### Downloads por channel

```{r plot_channel_downloads, fig.width=16, fig.height=5}
ggplot(data = tbl_undersample) +
  geom_histogram(mapping = aes(x = channel),
                 bins = 1,
                 fill = '#004d05') +
  scale_x_continuous(position = 'top') +
  theme(panel.grid = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) +
  xlab('Channel') + ylab('Downloads') +
  ggtitle('Downloads por channel') + 
  facet_grid(is_attributed ~ channel)
```

### Downloads por hora do dia

```{r plot_hour_downloads, fig.width=16, fig.height=5}
ggplot(data = tbl_undersample) +
  geom_histogram(mapping = aes(x = click_hour),
                 bins = 1,
                 fill = '#004d05') +
  scale_x_continuous(position = 'top') +
  theme(panel.grid = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) +
  xlab('Hora') + ylab('Downloads') +
  ggtitle('Downloads por hora do dia') + 
  facet_grid(is_attributed ~ click_hour)
```

* Alguns apps e channels obtém mais sucesso que os demais ao induzir um usuário a clicar e efetuar download. Percebemos isso observando a variação entre os IDs;

* Apesar da variedade de dispositivos, a grande maioria dos cliques originaram do device 1. Há pouca variação entre downloads realizados por clique, sugerindo que o dispositivo do usuário não influencia em sua decisão de efetuar o download. O device 0 possui participação significativa entre os dispositivos que efetuaram download, entretanto, não obtivemos amostras no grupo oposto para verificar a variabilidade deste registro;

* Semelhante aos dispositivos, o sistema operacional do usuário possui pouca variação quando comparado às classes previstas, sugerindo que não há influência dessa variável ao determinar a realização de um download;

* O horário do clique também possui pouca variação, sugerindo que essa variável não influencia na decisão do usuário em realizar download do app.

```{r cleanup_2, include=FALSE, echo=FALSE}
rm(list = c('tbl_dl_app','tbl_dl_device','tbl_dl_os','tbl_dl_channel',
            'tbl_click_app','tbl_click_device','tbl_click_os','tbl_click_channel',
            'tbl_train_sample', 'tbl_undersample', 'tbl_undersample_corr'))
```

# Concepção do modelo preditivo

Mesmo com as observações apontadas durante a análise, optei por utilizar todas as variáveis para conceber o modelo preditivo. Durante a criação, utilizei algumas técnicas para escolher quais variáveis seriam mantidas ou descartadas.

## Carregando dados para treino

Precisamos carregar mais dados para treinar o modelo preditivo, entretanto, é inviável carregar todo este dataset em memória na minha estação de trabalho:

```{r train_line_count}
system('wc -l ../input/train.csv')
```

Existem aproximadamente 185 milhões de registros disponíveis para treino. Optei por extrair 9 partições de 2.000.000 registros para cada 10% deste documento:

```{r train_select, warning=FALSE, message=FALSE}
#Índices de leitura do arquivo de treino
start_row_index <- 184903891 * seq(0.1, 0.9, 0.1)

#Função para agrupar uma coleção de tibbles
merge_inputs_by_row = function(..., is_attributed = NA){
  if(is.na(is_attributed)){
    return(bind_rows(...))
  }
  return(bind_rows(...) %>% filter(is_attributed == !!is_attributed))
}

#Extraindo partições a partir de cada índice
data_list <- lapply(start_row_index, function(x){
  read_csv(paste(input_dir, files[5], sep = '/'),
           n_max = 2000000, skip = x, col_names = c('ip', 'app', 'device', 'os',
                                                    'channel', 'click_time', 'attributed_time',
                                                    'is_attributed'))
})

#Agrupando os dados
tbl_train <- merge_inputs_by_row(data_list)

#Excluindo a lista, para disponibilizar mais espaço em memória
rm('data_list')
```

```{r class_distribution}
table(tbl_train$is_attributed)
```

Semelhante ao processo realizado durante análise, o modelo preditivo será treinado com um dataset balanceado:

```{r model_train_sample}
set.seed(5)
train_sample <- tbl_train %>%
  filter(is_attributed == 1)

train_sample <- bind_rows(train_sample,
                          tbl_train %>% 
                            filter(is_attributed == 0) %>% 
                            sample_n(nrow(train_sample)))
```

## Cálculo de features

Com o dataset devidamente balanceado, calculamos as features de downloads, cliques e data e hora:

```{r model_features}
train_sample <- train_sample %>% 
  select(-ip, -attributed_time) %>%
  strip_date()

#Features baseadas em downloads
tbl_dl_app <- mean_app_downloads(train_sample)
tbl_dl_device <- mean_device_downloads(train_sample)
tbl_dl_os <- mean_os_downloads(train_sample)
tbl_dl_channel <- mean_channel_downloads(train_sample)

#Features baseadas em cliques
tbl_click_app <- mean_app_clicks(train_sample)
tbl_click_device <- mean_device_clicks(train_sample)
tbl_click_os <- mean_os_clicks(train_sample)
tbl_click_channel <- mean_channel_clicks(train_sample)

#Realizado os cálculos, colocamos a variável target como fator
train_sample <- train_sample %>%
  mutate(is_attributed = as.factor(is_attributed))

#Agrupando os dados extraídos
train_sample <- train_sample %>%
  left_join(tbl_dl_app, by = 'app') %>%
  left_join(tbl_dl_device, by = 'device') %>%
  left_join(tbl_dl_os, by = 'os') %>%
  left_join(tbl_dl_channel, by = 'channel') %>%
  left_join(tbl_click_app, by = 'app') %>%
  left_join(tbl_click_device, by = 'device') %>%
  left_join(tbl_click_os, by = 'os') %>%
  left_join(tbl_click_channel, by = 'channel') %>%
  select(-app, -device, -os, -channel)
```

## Criação do modelo

Escolhi resolver este problema utilizando um algoritmo de regressão logística:

```{r train_test_split}
t_index = createDataPartition(train_sample$is_attributed, times = 1, p = 0.7, list = F)
train = train_sample[t_index,]
test = train_sample[-t_index,]
```

```{r glm_formula}
global_formula = formula(is_attributed ~ .)
glm_model = glm(global_formula, data = train, family = 'binomial')
step_model = step(glm_model, direction = 'both')
```

```{r step_summary}
summary(step_model)
```

A melhor fórmula calculada pela função step sugere a utilização de todas as médias calculadas em conjunto com o dia e hora do clique. A média de cliques por canal e o dia do clique apresentaram um baixo grau de significância, sugerindo pouca influência no modelo preditivo. 

## Validação do modelo

Será construída uma nova versão deste modelo, utilizando a fórmula sugerida pela função step. Também será utilizado cross-validation para buscar a melhor combinação de dados de treino:

```{r step_formula}
step_formula = formula(is_attributed ~ click_hour + mean_app_downloads + 
                         mean_device_downloads + mean_os_downloads + mean_channel_downloads + 
                         mean_app_clicks + mean_device_clicks + mean_os_clicks)
tr_c = trainControl(method = 'repeatedcv', number = 10, repeats = 10)
step_model = train(step_formula, data = train, method = 'glm', family = 'binomial', trControl = tr_c)
```

Com o modelo treinado, podemos avaliar a importância de cada variável:

```{r model_importance}
plot(varImp(step_model, scale=F))
```

Este gráfico demonstra mais precisamente o grau de importância de cada variável. Futuramente, podemos criar novas versões do modelo preditivo utilizando combinações diferentes dessas variáveis.

Concluído o treinamento do modelo, faremos as previsões sobre os dados de teste deste conjunto:

```{r confusion_matrix}
step_preds = predict.train(step_model, test)
confusionMatrix(step_preds, test$is_attributed, positive = '1')
```

Este modelou apresentou 91% de acurácia (capacidade de classificar corretamente o valor previsto) e 86% de sensibilidade (capacidade de classificar corretamente a ocorrência de downloads).

Entretanto, estes valores foram obtidos a partir de um dataset balanceado. Sabemos que é esperado um volume muito menor de downloads por clique e precisamos testar este modelo com novos dados.

## Teste do modelo

Serão extraídos aleatoriamente registros do dataset original. Essa abordagem possui alguns problemas:

* Apesar da aleatoriedade, é possível que sejam selecionados registros já observados durante o treinamento;

* É provável que sejam selecionados IDs não observados durante o treinamento, portanto, o cálculo da média de registros não observados anteriormente será NA.

Para solucionar o primeiro problema, optei por extrair um conjunto de dados pelo menos 10 vezes maior que o conjunto utilizado para treino. Dessa forma, mesmo que todos registros observados durante o treinamento sejam aleatoriamente selecionados, ainda existirá um grande volume de dados não observados.

Para o segundo problema, optei por imputar um valor constante igual a 0 visto que não conhecemos a média de cliques ou downloads dos IDs não observados:

```{r unbalanced_data_test}
set.seed(11)
tbl_unbalanced_test = tbl_train %>% sample_n(1000000)

#Atribuindo features baseadas no dataset de treino
tbl_unbalanced_test <- tbl_unbalanced_test %>% 
  select(-ip, -attributed_time) %>%
  mutate(is_attributed = as.factor(is_attributed)) %>%
  strip_date() %>%
  left_join(tbl_dl_app, by = 'app') %>%
  left_join(tbl_dl_device, by = 'device') %>%
  left_join(tbl_dl_os, by = 'os') %>%
  left_join(tbl_dl_channel, by = 'channel') %>%
  left_join(tbl_click_app, by = 'app') %>%
  left_join(tbl_click_device, by = 'device') %>%
  left_join(tbl_click_os, by = 'os') %>%
  left_join(tbl_click_channel, by = 'channel') %>%
  select(-app, -device, -os, -channel) %>%
  replace(is.na(.), 0)
```

Com novos dados preparados, testamos o modelo novamente:

```{r confusion_matrix_2}
unbalanced_preds = predict.train(step_model, tbl_unbalanced_test)
confusionMatrix(unbalanced_preds, tbl_unbalanced_test$is_attributed, positive = '1')
```

O modelo obteve 96% de acurácia ao ser testado com dados inéditos, sugerindo que não ocorreu *overfitting* durante o treinamento e sua capacidade de generalização foi preservada.

Entretanto, o modelo previu a ocorrência de 34143 downloads, quando na realidade houveram apenas 2724. Isso sugere que o modelo está bastante sensível a classificar incorretamente a ocorrência de downloads.

## Submetendo resultados ao Kaggle

Para concluir este trabalho, utilizei este mesmo modelo e médias calculadas para prever os dados de teste da competição:

![kaggle_submission.png](../output/kaggle_submission.png)

Foi utilizada a métrica AUC para esta competição. A melhoria na pontuação do Private Score sugere que o modelo manteve a capacidade de generalização.

```{r cleanup_final, include=FALSE, echo=FALSE}
rm(list = ls())
```